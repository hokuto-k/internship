import maf
import maflib.util
import maflib.rules
import os
import random
import mocha.net
import subprocess
import shutil
import Image
import caffe
import numpy as np
import matplotlib.pyplot as plt
import re
import json
import string
import caffe.proto.caffe_pb2
import google.protobuf.text_format

def options(opt):
	pass

def configure(conf):
	pass

def build(exp): 
	qmul_dir = "/home/hokutokagaya/exp/QMUL_head_detect/QMULPoseHeads/"
	dataset_original_train = qmul_dir + "train/"
	dataset_original_test = qmul_dir + "test/"
	dataset_dir_train = "data_resized_train/"
	dataset_dir_test = "data_resized_test/"
	dataset_adding_mirrored = "dataset_adding_mirrored/"
	# cifar10_pretrained = "cifar10_full_iter_70000"

	# Resize
	exp(source=dataset_original_train,
		target=dataset_dir_train,
		parameters=[{"resize_size":32}],
		rule=resize_images()
		)

	exp(source=dataset_original_test,
		target=dataset_dir_test,
		parameters=[{"resize_size":32}],
		rule=resize_images()
		)


	# Make datasets available for convert_imageset.bin
	exp(source=dataset_dir_train,
		target=["train.txt"],
		rule=make_lists()
		)

	exp(source=dataset_dir_test,
		target=["test.txt"],
		rule=make_lists()
		)


	# Make datasets available for Caffe using convert_imageset.bin
	exp(source=["train.txt"],
		target="train_leveldb",
		rule='GLOG_logtostderr=1 convert_imageset.bin / ${SRC[0].abspath()} ${TGT} 1')

	exp(source=["test.txt"],
		target="test_leveldb",
		rule='GLOG_logtostderr=1 convert_imageset.bin / ${SRC[0].abspath()} ${TGT} 1')

	# Compute the mean of images
	exp(source="train_leveldb",
		target="mean.binaryproto",
		rule="compute_image_mean.bin ${SRC} ${TGT}")

	# Make prototxts
	exp(source="base_2layer.prototxt",
		target="changed_base.prototxt",
		parameters=maflib.util.product({
			# "conv1-filtersize":[5],
			# "conv2-filtersize":[2],
			# "conv1-num_output":[32],
			# "conv2-num_output":[32],
			# "pool1-k_size":[2],
			# "pool2-k_size":[2],
			"q":[1]
			}),
		rule=change_param())

	# exp(source=["data.prototxt", "changed_base.prototxt", "loss.prototxt"],
	# 	target="train_base.prototxt",
	# 	rule="cat ${SRC} > ${TGT}")

	# exp(source=["train_base.prototxt", "train_leveldb", "mean.binaryproto"],
	# 	target="train.prototxt",
	# 	rule=mocha.net.configure_data_layer(batchsize=256))

	# exp(source=["data.prototxt", "changed_base.prototxt", "loss_and_ac.prototxt"],
	# 	target="test_base.prototxt",
	# 	rule="cat ${SRC} > ${TGT}")

	# exp(source=["test_base.prototxt", "test_leveldb", "mean.binaryproto"],
	# 	target="test.prototxt",
	# 	rule=mocha.net.configure_data_layer(batchsize=550))

	exp(source=["data.prototxt", "changed_base.prototxt", "loss_and_ac.prototxt"],
		target="train_and_test_base.prototxt",
		rule="cat ${SRC} > ${TGT}")

	exp(source=["train_and_test_base.prototxt", "train_leveldb", "test_leveldb", "mean.binaryproto"],
		target="train_and_test.prototxt",
		rule=mocha.net.configure_data_layer(train_batchsize=256, test_batchsize=1000))

	# Train nets
	exp(source='train_and_test.prototxt',
	            target='solver.prototxt',
	            parameters=maflib.util.product({
	                'base_lr': [0.001],
	                'momentum': [0.9],
	                'weight_decay': [0.004],
	                'lr_policy': ["step"],
	                'gamma': [0.1],
	                'stepsize': [1000],
	                'max_iter': [4000],
	                'snapshot': [4000],
	                # 'snapshot_prefix': 'bmvc2009'
	            }),
	            rule=mocha.net.create_solver(
	                test_iter=5, test_interval=1000, display=100))
	
	exp(source=["solver.prototxt"],
		target=["log.txt", "snapshots", 'final_model'],
		rule=caffe_train())

	# Make prototxt for deploy
	# exp(source=["data_deploy.prototxt", "changed_base.prototxt", "prob.prototxt"],
	# 	target="deploy.prototxt",
	# 	rule="cat ${SRC} > ${TGT}")

	# Make mean.binaryproto mean.npy
	exp(source="mean.binaryproto",
		target="mean.npy",
		rule=mean2npy())

	# # Compute angular error
	# exp(source=["final_model","deploy.prototxt","test.txt","mean.npy"],
	# 	target="result.txt",
	# 	rule=compute_angle_error())

	# exp(source="result.txt",
	# 	target="resuclt.average",
	# 	aggregate_by='p',
	# 	rule=maflib.rules.average)

	# ecp(source="result.txt",
	# 	target="plot.png",
	# 	parameter=,
	# 	rule=my_plot)


@maflib.util.rule
def caffe_train(task):
	output_dir = task.outputs[1].abspath()

	try:
		os.makedirs(output_dir)
	except:
		pass

	f = open(task.inputs[0].abspath(), "r")
	base_solver = f.read()
	f.close()

	solver = output_dir + "/solver.prototxt"
	f = open(solver, "w")
	f.write(base_solver)
	f.write("snapshot_prefix: \"%s/\" \n" % output_dir)
	f.close()

	envs = dict(os.environ)
	envs["GLOG_logtostderr"] = str(1)


	subprocess.check_call(["caffe.bin", "train", "-solver", solver], env=envs,
		stderr=open(task.outputs[0].abspath(), "w"))

	shutil.copyfile(output_dir+'/_iter_4000', task.outputs[2].abspath())

@maflib.util.rule
def make_lists(task):

	#param
	pre_file_list = []
	tr_list = []
	val_list = []
	test_list = []
	dir_list = []
	
	path = task.inputs[0].abspath()
	for root, dirs, files in os.walk(path):
		for dir in dirs:
			print dir
			dir_list.append(dir)

	dir_list.sort()


	
	for dir in dir_list:
		if 'bg' in dir:
			class_num = 0
		else:
			class_num = 1
		path2 = os.path.join(path, dir)
		print path2
		for root, dirs, files in os.walk(path2):
			for file_ in files:
				print file_
				filename = os.path.join(root, file_)
				pre_file_list.append([filename, class_num])

		

	random.shuffle(pre_file_list)

	list_file = open(task.outputs[0].abspath(), 'w')
	
	for instance in pre_file_list:
		list_file.write(instance[0] + " " + str(instance[1]) + "\n")


@maflib.util.rule
def resize_images(task):
	i = 0
	path = task.inputs[0].abspath()

	#指定先ディレクトリがなければ作成
	dst_dir = task.outputs[0].abspath()
	if os.path.isdir(dst_dir) == False:
		os.makedirs(dst_dir)

	size = task.parameter['resize_size']
	for root, dirs, files in os.walk(path):
		for dir in dirs:
			dst_dir_class = os.path.join(dst_dir, dir)
			if os.path.isdir(dst_dir_class) == False:
				os.makedirs(dst_dir_class)
			for root2, dirs2, files2 in os.walk(os.path.join(path, dir)):
				for file in files2:
					root3, ext = os.path.splitext(file)
					if ext == '.jpg' or ext == '.JPG' or ext == '.png' or ext == '.PNG':
						
						try:
							img = Image.open(os.path.join(root2, file))
						except:
							print str(file) + "can't open!"
							continue

						file = file.replace(' ', '')
						if img.mode != "RGB":
							img = img.convert("RGB")
						img.resize((size, size)).save(dst_dir+"/"+dir+"/"+file) 

						#print os.path.join(root, file)
						i += 1
						if i % 100 == 0:
							print str(i) + "now.."

@maflib.util.rule
def mean2npy(task):
	blob = caffe.proto.caffe_pb2.BlobProto()
	data = open( task.inputs[0].abspath() , 'rb' ).read()
	blob.ParseFromString(data)
	arr = np.array( caffe.io.blobproto_to_array(blob) )
	out = arr[0]
	np.save( task.outputs[0].abspath() , out )

@maflib.util.rule
def compute_angle_error(task):

	# caffe_root = '../../'  # this file is expected to be in {caffe_root}/examples
	# bmvc_root = caffe_root + 'examples/BMVC2009/'
	MODEL_FILE = task.inputs[1].abspath()
	PRETRAINED = task.inputs[0].abspath()


	net = caffe.Classifier(MODEL_FILE, PRETRAINED,
	                       mean_file=task.inputs[3].abspath(),
	                       channel_swap=(2,1,0),
	                       input_scale=255,
	                       gpu=True)

	f_test = open(task.inputs[2].abspath())
	test_list = []
	ans_list = []
	angle_list = []
	num = 0
	angle_error1 = 0
	angle_error2 = 0


	r = re.compile('-[0-9]+')
	for line in f_test:
		filename = line.split()[0]
		test_list.append(filename)
		ans_list.append(int(line.split()[1]))
		p = r.search(filename)
		angle_list.append(float(filename[p.start()+1:p.end()]))

	for IMAGE_FILE in test_list:
		input_image = caffe.io.load_image(IMAGE_FILE)
		prediction = net.predict([input_image], oversample=False)
		predict_class = np.argmax(prediction[0])
		predict_angle1 = predict_class * 45.0 + 22.5
		predict_angle2 = 0
		for i in range(8):
			predict_angle2 += prediction[0][i] * (i * 45.0 + 22.5) 

		if ans_list[test_list.index(IMAGE_FILE)] == predict_class:
			# print "Correct!"
			num += 1
		else:
			# print "Wrong!"
			pass		

		ae1 = abs(predict_angle1 - angle_list[test_list.index(IMAGE_FILE)])
		ae2 = abs(predict_angle2 - angle_list[test_list.index(IMAGE_FILE)])
		if ae1 < 180:
			angle_error1 += ae1
		else:
			angle_error1 += 360 - ae1 

		if ae2 < 180:
			angle_error2 += ae2
		else:
			angle_error2 += 360 - ae2


	angle_error1 = angle_error1 / float(len(test_list))
	angle_error2 = angle_error2 / float(len(test_list))

	out = open(task.outputs[0].abspath(), 'w')
	out.write(json.dumps({'accuracy':float(num) / len(test_list), 'angle error 1':angle_error1, 'angle error 2':angle_error2}, sort_keys=True, indent=4))
	# print "accuracy:", float(num) / len(test_list)
	# print "angle error 1 (simple way):", angle_error1 
	# print "angle error 2 (weighted average):", angle_error2

@maflib.plot.plot_by
def my_plot(figure, data, parameter):
    # キー 'a', 'b' に対応するリストを取り出す
    x, y = data.get_data_2d('a', 'b')

    # これをプロット
    axes = figure.add_subplot(111)
    axes.plot(x, y)


@maflib.util.rule
def change_param(task):
	net_str = task.inputs[0].read()
	net = caffe.proto.caffe_pb2.NetParameter()
	google.protobuf.text_format.Merge(net_str, net)

	for layer in net.layers:
		if layer.name == 'pool1':
			layer_p1 = layer
		elif layer.name == 'pool2':
			layer_p2 = layer
		elif layer.name == 'pool3':
			layer_p3 = layer
		elif layer.name == 'conv1':
			layer_c1 = layer
		elif layer.name == 'conv2':
			layer_c2 = layer
		elif layer.name == 'conv3':
			layer_c3 = layer

	# layer_p1.pooling_param.kernel_size = task.parameter["pool1-k_size"]
	# layer_p2.pooling_param.kernel_size = task.parameter["pool2-k_size"]
	result = google.protobuf.text_format.MessageToString(net)
	task.outputs[0].write(result)



